{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load,dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inbound(inbound:list)->str:\n",
    "    return ( \"([\" + \", \".join(inbound) + \"])\" if len(inbound) > 1 else \"(\" + inbound[0] + \")\" ) if len(inbound) else \"\"\n",
    "\n",
    "def set_argument(argument,config):\n",
    "    value = config['value']\n",
    "    try:\n",
    "        value = eval(value)\n",
    "    except:\n",
    "        pass\n",
    "    value = ( None if value == 'None' else f\"'{value}'\" ) if config['type'] == 'text' else value\n",
    "    return f\"    {argument}={value.__repr__()},\"\n",
    "    \n",
    "def build_arguments(arguments:dict)->str:\n",
    "    arguments = '\\n'.join([ set_argument(arg,cnf) for arg,cnf in arguments.items() ])\n",
    "    return arguments\n",
    "\n",
    "def build_default(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound']) if layer['type']['name'] != 'Input' else '' \n",
    "    \n",
    "    return f\"\"\"{layer['id']} = {layer['type']['_class']}.{layer['type']['name']}(\n",
    "{arguments}\n",
    "){inbound} #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_application(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound']) if layer['type']['name'] != 'Input' else '' \n",
    "    return f\"\"\"{layer['id']} = applications.{layer['type']['_class']}(\n",
    "    input_tensor={layer['connections']['inbound'][0]},\n",
    "{arguments}\n",
    "    include_top=False\n",
    ").output #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_model(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['model'] = layer\n",
    "    return f\"\"\"{layer['id']} = keras.Model(\n",
    "    [ {', '.join(build_config['input_nodes'])}, ],\n",
    "    [ {', '.join(layer['connections']['inbound'])}, ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_compile(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['compile'] = layer\n",
    "    model,*_ = [node for node in layer['connections']['inbound'] if \"model\" in node]\n",
    "    metrics = layer['arguments']['metrics']['value']\n",
    "    metrics = \"[\\\"\" + '\", \"'.join(metrics) + \"\\\"]\" if len(metrics) else 'None'\n",
    "    \n",
    "    train_config = build_config['train_config']\n",
    "    return f\"\"\"{train_config['optimizer']['value'] if train_config['optimizer'] else ''}\n",
    "{model}.compile(\n",
    "    optimizer={train_config['optimizer']['id'] if train_config['optimizer'] else \"'\"+layer['arguments']['optmizer']['value']+\"'\"},\n",
    "    loss='{layer['arguments']['loss']['value']}',\n",
    "    metrics={metrics}\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_train(layer,build_config,*args,**kwargs)->str:\n",
    "    callbacks = [callback['value'] for callback in build_config['train_config']['callbacks']]\n",
    "    callback_ids = [callback['id'] for callback in build_config['train_config']['callbacks']]\n",
    "    build_config['train_config']['train'] = layer\n",
    "        \n",
    "#     print (build_config['train_config']['model'])\n",
    "        \n",
    "    return f\"\"\"{\"\".join(callbacks)}\n",
    "{build_config['train_config']['model']['id']}.fit(\n",
    "    x={build_config['train_config']['dataset']['id']}.train_x,\n",
    "    y={build_config['train_config']['dataset']['id']}.train_y,\n",
    "    batch_size={layer['arguments']['batch_size']['value']},\n",
    "    epochs={layer['arguments']['epochs']['value']},\n",
    "    validation_data=( {build_config['train_config']['dataset']['id']}.test_x, {build_config['train_config']['dataset']['id']}.test_y ),\n",
    "    callbacks=[ tfgui, {', '.join(callback_ids)} ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "build_functions = {\n",
    "    \"default\":build_default,\n",
    "    \"Model\":build_model,\n",
    "    \"Compile\":build_compile,\n",
    "    \"Train\":build_train,\n",
    "    \"Application\":build_application\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../temp/unet.json\",\"r\") as build_file:\n",
    "    build_config = load(build_file)\n",
    "    \n",
    "inputs = []\n",
    "train_config = {\n",
    "    \"dataset\":None,\n",
    "    \"optimizer\":None,\n",
    "    \"loss\":None,\n",
    "    \"callbacks\":[],\n",
    "    \n",
    "    \"model\":None,\n",
    "    \"compile\":None,\n",
    "    \"train\":None\n",
    "}\n",
    "\n",
    "\n",
    "for _id,config in build_config.items():\n",
    "#     print (config['type']['name'])\n",
    "    if config['type']['name'] == 'Input':\n",
    "        inputs.append(_id)\n",
    "        \n",
    "    elif config['type']['name'] == 'Dataset':\n",
    "        train_config['dataset'] = _id\n",
    "             \n",
    "    elif config['type']['name'] == 'Model':\n",
    "        train_config['model'] = _id\n",
    "        \n",
    "    elif config['type']['name'] == 'Loss':\n",
    "        train_config['loss'] = _id\n",
    "    \n",
    "    if config['type']['_class'] == 'optimizers':\n",
    "        train_config['optimizer'] = _id\n",
    "        \n",
    "    elif config['type']['_class'] == 'callbacks':\n",
    "        train_config['callbacks'].append(_id)\n",
    "\n",
    "build_config['train_config'] = train_config\n",
    "build_config['input_nodes'] = inputs\n",
    "\n",
    "skip = []\n",
    "levels = [ set() for i in range(len(build_config))]\n",
    "\n",
    "def setLevel(node,config,di=0):\n",
    "    if node not in skip:\n",
    "        levels[di].add(node)\n",
    "        skip.append(node)\n",
    "    \n",
    "    if build_config[node]['connections']['outbound']:\n",
    "        for next_node in build_config[node]['connections']['outbound']: \n",
    "            setLevel(next_node,build_config,di+1)\n",
    "\n",
    "for inp in inputs:\n",
    "    setLevel(inp,build_config,0)\n",
    "    \n",
    "levels = [list(level) for level in levels if len(level)]    \n",
    "for level in levels:\n",
    "    for idx,layer in enumerate(level):\n",
    "        for jdx,conn in enumerate(level[idx+1:]):\n",
    "            if conn in build_config[layer]['connections']['inbound']:\n",
    "                level[jdx+idx+1],level[idx] = layer,conn\n",
    "            \n",
    "build_config['levels'] = levels\n",
    "for key,val in train_config.items():\n",
    "    if val != None:\n",
    "        if key == 'dataset':\n",
    "            train_config['dataset'] = {\n",
    "                \"id\":val,\n",
    "                \"value\":build_config[val]['arguments']['dataset']['value']\n",
    "            }\n",
    "        elif key == 'optimizer':\n",
    "            train_config['optimizer'] = {\n",
    "                'id':val,\n",
    "                'value':build_default(build_config[val],build_config)+'\\n'\n",
    "            }\n",
    "        elif key == 'callbacks':\n",
    "            callbacks = []\n",
    "            for callback in train_config['callbacks']:\n",
    "                callbacks.append({\n",
    "                    \"id\":callback,\n",
    "                    \"value\":build_default(build_config[callback],build_config)\n",
    "                })\n",
    "            train_config['callbacks'] = callbacks\n",
    "            \n",
    "build = ''\n",
    "\n",
    "for level in levels:\n",
    "    for layer in level:\n",
    "        layer = build_config[layer]\n",
    "        if layer['type']['name'] in build_functions: \n",
    "            build += build_functions[layer['type']['name']](layer,build_config) + '\\n\\n'\n",
    "        else:\n",
    "            build += build_functions['default'](layer,build_config) + '\\n\\n'\n",
    "            \n",
    "build = build[:-2]\n",
    "code = \"\"\"#-*- Code generated by Tensorflow GUI -*-\n",
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications\n",
    "\n",
    "#end-import\n",
    "\n",
    "{dataset}\n",
    "{build}\n",
    "\"\"\".format(\n",
    "    dataset=train_config['dataset']['value'],\n",
    "    build=build\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['input_2'],\n",
       " ['conv2d_6'],\n",
       " ['conv2d_7'],\n",
       " ['dropout_1'],\n",
       " ['conv2d_8'],\n",
       " ['conv2d_9'],\n",
       " ['dropout_2'],\n",
       " ['conv2d_10'],\n",
       " ['conv2d_11'],\n",
       " ['dropout_3'],\n",
       " ['conv2d_12'],\n",
       " ['conv2d_13'],\n",
       " ['dropout_4'],\n",
       " ['conv2d_14'],\n",
       " ['conv2d_15'],\n",
       " ['dropout_5'],\n",
       " ['conv2dtranspose_1'],\n",
       " ['concatenate_3'],\n",
       " ['conv2dtranspose_2'],\n",
       " ['conv2dtranspose_3'],\n",
       " ['dropout_6'],\n",
       " ['concatenate_4'],\n",
       " ['conv2dtranspose_4'],\n",
       " ['conv2dtranspose_5'],\n",
       " ['dropout_7'],\n",
       " ['concatenate_5'],\n",
       " ['conv2dtranspose_6'],\n",
       " ['conv2dtranspose_7'],\n",
       " ['dropout_8'],\n",
       " ['concatenate_6'],\n",
       " ['conv2dtranspose_8'],\n",
       " ['conv2dtranspose_9'],\n",
       " ['dropout_9'],\n",
       " ['dense_1'],\n",
       " ['model_1'],\n",
       " ['compile_1'],\n",
       " ['train_1']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_train.json\",\"w+\") as file:\n",
    "    file.write(build_config.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-*- Code generated by Tensorflow GUI -*-\n",
      "#import\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications\n",
      "\n",
      "#end-import\n",
      "\n",
      "\"\"\"\n",
      "Note : Don't change dataset id.\n",
      "\n",
      "All the required packages have been imported with their standard namespaces.\n",
      "\n",
      "tensorflow as tf\n",
      "keras as keras\n",
      "pandas as pd\n",
      "numpy as np\n",
      "\n",
      "from sklearn.model_selection , train_test_split\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "#dataset id=dataset_1\n",
      "class Dataset:\n",
      "    \"\"\"\n",
      "    Dataset will be used in training \n",
      "\n",
      "    The dataset object needs to have following attributes\n",
      "\n",
      "    train_x : np.ndarray -> Training features\n",
      "    train_y : np.ndarray -> Training labels \n",
      "    test_x : np.ndarray -> Testing features\n",
      "    test_y : np.ndarray -> Testing labels\n",
      "\n",
      "    validate : bool -> Weather use validation data or not\n",
      "\n",
      "    batch_size : int -> Batch size\n",
      "    epochs : int -> Number of epochs\n",
      "    batches : int -> Number of batches ( Will be calculated automatically )\n",
      "    \"\"\"\n",
      "    train_x = None\n",
      "    test_x = None\n",
      "    train_y = None\n",
      "    test_y = None\n",
      "\n",
      "    validate = True\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        \"\"\"\n",
      "        Load dataset and set required variables.\n",
      "        \"\"\"\n",
      "\n",
      "        images = glob(\"C:\\\\workspace\\\\tensorflow-gui\\\\data\\\\datasets\\\\bfsiw\\\\leedsbutterfly\\\\images\\\\*\")\n",
      "        labels = glob(\"C:\\\\workspace\\\\tensorflow-gui\\\\data\\\\datasets\\\\bfsiw\\\\leedsbutterfly\\\\segmentations\\\\*\")\n",
      "\n",
      "    \n",
      "        self.train_x = np.zeros((len(images),224,224,3)).astype(np.float32)\n",
      "        self.train_y = np.zeros((len(labels),224,224,3)).astype(np.float32)\n",
      "\n",
      "        def get_image(args):\n",
      "            index,path,array = args\n",
      "            im = cv2.imread(path,)[:,:,::-1]\n",
      "            im = cv2.resize(im,(224,224),interpolation=cv2.INTER_AREA)\n",
      "            array[index] = im\n",
      "            return 1\n",
      "\n",
      "\n",
      "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
      "            res = executor.map(get_image,[ ( i,path,self.train_x ) for i,path in enumerate(images)])\n",
      "            \n",
      "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
      "            res = executor.map(get_image,[ ( i,path,self.train_y ) for i,path in enumerate(labels)])\n",
      "\n",
      "        self.train_y = self.train_y.mean(axis=-1) / 255\n",
      "        test_idx = np.random.randint(0,len(self.train_x),size=32)\n",
      "        \n",
      "        self.test_x = self.train_x[test_idx]\n",
      "        self.test_y = self.train_y[test_idx]\n",
      "\n",
      "        collect()\n",
      "        \n",
      "# Do not change the anything.\n",
      "dataset_1 = Dataset()\n",
      "#end-dataset id=dataset_1\n",
      "input_2 = layers.Input(\n",
      "    shape=(224, 224, 3),\n",
      "    batch_size=None,\n",
      "    name=None,\n",
      "    dtype=None,\n",
      "    sparse=False,\n",
      "    tensor=None,\n",
      "    ragged=False,\n",
      ") #end-input_2\n",
      "\n",
      "\n",
      "conv2d_6 = layers.Conv2D(\n",
      "    filters=8,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(input_2) #end-conv2d_6\n",
      "\n",
      "\n",
      "conv2d_7 = layers.Conv2D(\n",
      "    filters=8,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_6) #end-conv2d_7\n",
      "\n",
      "\n",
      "dropout_1 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2d_7) #end-dropout_1\n",
      "\n",
      "\n",
      "conv2d_8 = layers.Conv2D(\n",
      "    filters=16,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_1) #end-conv2d_8\n",
      "\n",
      "\n",
      "conv2d_9 = layers.Conv2D(\n",
      "    filters=16,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_8) #end-conv2d_9\n",
      "\n",
      "\n",
      "dropout_2 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2d_9) #end-dropout_2\n",
      "\n",
      "\n",
      "conv2d_10 = layers.Conv2D(\n",
      "    filters=32,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_2) #end-conv2d_10\n",
      "\n",
      "\n",
      "conv2d_11 = layers.Conv2D(\n",
      "    filters=32,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_10) #end-conv2d_11\n",
      "\n",
      "\n",
      "dropout_3 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2d_11) #end-dropout_3\n",
      "\n",
      "\n",
      "conv2d_12 = layers.Conv2D(\n",
      "    filters=64,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_3) #end-conv2d_12\n",
      "\n",
      "\n",
      "conv2d_13 = layers.Conv2D(\n",
      "    filters=64,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_12) #end-conv2d_13\n",
      "\n",
      "\n",
      "dropout_4 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2d_13) #end-dropout_4\n",
      "\n",
      "\n",
      "conv2d_14 = layers.Conv2D(\n",
      "    filters=128,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_4) #end-conv2d_14\n",
      "\n",
      "\n",
      "conv2d_15 = layers.Conv2D(\n",
      "    filters=128,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_14) #end-conv2d_15\n",
      "\n",
      "\n",
      "dropout_5 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2d_15) #end-dropout_5\n",
      "\n",
      "\n",
      "conv2dtranspose_1 = layers.Conv2DTranspose(\n",
      "    filters=64,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_5) #end-conv2dtranspose_1\n",
      "\n",
      "\n",
      "concatenate_3 = layers.Concatenate(\n",
      "\n",
      ")([conv2dtranspose_1, dropout_4]) #end-concatenate_3\n",
      "\n",
      "\n",
      "conv2dtranspose_2 = layers.Conv2DTranspose(\n",
      "    filters=64,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(concatenate_3) #end-conv2dtranspose_2\n",
      "\n",
      "\n",
      "conv2dtranspose_3 = layers.Conv2DTranspose(\n",
      "    filters=32,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2dtranspose_2) #end-conv2dtranspose_3\n",
      "\n",
      "\n",
      "dropout_6 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2dtranspose_3) #end-dropout_6\n",
      "\n",
      "\n",
      "concatenate_4 = layers.Concatenate(\n",
      "\n",
      ")([dropout_6, dropout_3]) #end-concatenate_4\n",
      "\n",
      "\n",
      "conv2dtranspose_4 = layers.Conv2DTranspose(\n",
      "    filters=32,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(concatenate_4) #end-conv2dtranspose_4\n",
      "\n",
      "\n",
      "conv2dtranspose_5 = layers.Conv2DTranspose(\n",
      "    filters=16,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2dtranspose_4) #end-conv2dtranspose_5\n",
      "\n",
      "\n",
      "dropout_7 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2dtranspose_5) #end-dropout_7\n",
      "\n",
      "\n",
      "concatenate_5 = layers.Concatenate(\n",
      "\n",
      ")([dropout_7, dropout_2]) #end-concatenate_5\n",
      "\n",
      "\n",
      "conv2dtranspose_6 = layers.Conv2DTranspose(\n",
      "    filters=16,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(concatenate_5) #end-conv2dtranspose_6\n",
      "\n",
      "\n",
      "conv2dtranspose_7 = layers.Conv2DTranspose(\n",
      "    filters=8,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2dtranspose_6) #end-conv2dtranspose_7\n",
      "\n",
      "\n",
      "dropout_8 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2dtranspose_7) #end-dropout_8\n",
      "\n",
      "\n",
      "concatenate_6 = layers.Concatenate(\n",
      "\n",
      ")([dropout_8, dropout_1]) #end-concatenate_6\n",
      "\n",
      "\n",
      "conv2dtranspose_8 = layers.Conv2DTranspose(\n",
      "    filters=8,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=(1, 1),\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(concatenate_6) #end-conv2dtranspose_8\n",
      "\n",
      "\n",
      "conv2dtranspose_9 = layers.Conv2DTranspose(\n",
      "    filters=3,\n",
      "    kernel_size=3,\n",
      "    padding='same',\n",
      "    strides=2,\n",
      "    output_padding=None,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    activation='swish',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2dtranspose_8) #end-conv2dtranspose_9\n",
      "\n",
      "\n",
      "dropout_9 = layers.Dropout(\n",
      "    rate=0.1,\n",
      "    noise_shape=None,\n",
      "    seed=None,\n",
      ")(conv2dtranspose_9) #end-dropout_9\n",
      "\n",
      "\n",
      "dense_1 = layers.Dense(\n",
      "    units=1,\n",
      "    activation='sigmoid',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(dropout_9) #end-dense_1\n",
      "\n",
      "\n",
      "model_1 = keras.Model(\n",
      "    [ input_2, ],\n",
      "    [ dense_1, ]\n",
      ") #end-model_1\n",
      "\n",
      "\n",
      "\n",
      "model_1.compile(\n",
      "    optimizer='adam',\n",
      "    loss='mean_squared_error',\n",
      "    metrics=[\"binary_crossentropy\", \"mean_absolute_error\", \"mean_squared_error\"]\n",
      ") #end-compile_1\n",
      "\n",
      "\n",
      "\n",
      "model_1.fit(\n",
      "    x=dataset_1.train_x,\n",
      "    y=dataset_1.train_y,\n",
      "    batch_size=8,\n",
      "    epochs=10,\n",
      "    validation_data=( dataset_1.test_x, dataset_1.test_y ),\n",
      "    callbacks=[ tfgui,  ]\n",
      ") #end-train_1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_code.py\",\"w+\") as file:\n",
    "    file.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
