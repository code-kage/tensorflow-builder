{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['input_1'],\n",
       " ['dense_1'],\n",
       " ['batchnormalization_1'],\n",
       " ['activation_1'],\n",
       " ['model_1'],\n",
       " ['compile_1'],\n",
       " ['train_1']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/example_build.json\",\"r\") as build_file:\n",
    "    build_config = load(build_file)\n",
    "    \n",
    "inputs = []\n",
    "train_config = {\n",
    "    \"dataset\":None,\n",
    "    \"optimizer\":None,\n",
    "    \n",
    "    \"model\":None,\n",
    "    \"compile\":None\n",
    "}\n",
    "\n",
    "\n",
    "for _id,config in build_config.items():\n",
    "    if config['type'] == 'Input':\n",
    "        inputs.append(_id)\n",
    "        \n",
    "    elif config['type'] == 'Dataset':\n",
    "        train_config['dataset'] = _id\n",
    "        \n",
    "    elif config['type'] == 'Optimizer':\n",
    "        train_config['optimizer'] = _id\n",
    "    \n",
    "build_config['input_nodes'] = inputs\n",
    "levels = [ set() for i in range(len(build_config))]\n",
    "def setLevel(node,config,di=0):\n",
    "    levels[di].add(node)\n",
    "    if build_config[node]['connections']['outbound']:\n",
    "        for next_node in build_config[node]['connections']['outbound']: \n",
    "            setLevel(next_node,build_config,di+1)\n",
    "\n",
    "\n",
    "for inp in inputs:\n",
    "    setLevel(inp,build_config,0)\n",
    "    \n",
    "build_config['levels'] = levels\n",
    "    \n",
    "levels = [list(level) for level in levels if len(level)]\n",
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inbound(inbound:list)->str:\n",
    "    return ( \"([\" + \", \".join(inbound) + \"])\" if len(inbound) > 1 else \"(\" + inbound[0] + \")\" ) if len(inbound) else \"\"\n",
    "\n",
    "def set_argument(argument,config):\n",
    "    value = config['value']\n",
    "    try:\n",
    "        value = eval(value)\n",
    "    except:\n",
    "        pass\n",
    "    value = ( None if value == 'None' else f\"'{value}'\" ) if config['type'] == 'text' else value\n",
    "    return f\"    {argument}={value.__repr__()},\"\n",
    "    \n",
    "def build_arguments(arguments:dict)->str:\n",
    "    arguments = '\\n'.join([ set_argument(arg,cnf) for arg,cnf in arguments.items() ])\n",
    "    return arguments\n",
    "\n",
    "def build_input(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    return f\"\"\"{layer['id']} = layers.Input(\n",
    "{arguments}\n",
    ")\"\"\"\n",
    "\n",
    "def build_default(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound'])\n",
    "    \n",
    "    return f\"\"\"{layer['id']} = layers.{layer['type']}(\n",
    "{arguments}\n",
    "){inbound}\"\"\"\n",
    "\n",
    "def build_model(layer,build_config,*args,**kwargs)->str:\n",
    "    train_config['model'] = layer\n",
    "    return f\"\"\"{layer['id']} = keras.Model(\n",
    "    [ {', '.join(build_config['input_nodes'])}, ],\n",
    "    [ {', '.join(layer['connections']['inbound'])}, ]\n",
    ")\"\"\"\n",
    "\n",
    "def build_compile(layer,build_config,*args,**kwargs)->str:\n",
    "    train_config['compile'] = layer\n",
    "    model,*_ = [node for node in layer['connections']['inbound'] if \"model\" in node]\n",
    "    return f\"\"\"{model}.compile(\n",
    "    optimizer={train_config['optimizer'] if train_config['optimizer'] else \"'\"+layer['arguments']['optmizer']['value']+\"'\"},\n",
    "    loss='{layer['arguments']['loss']['value']}'\n",
    ")\"\"\"\n",
    "\n",
    "def build_train(layer,build_config,*args,**kwargs)->str:\n",
    "    return \"\"\n",
    "\n",
    "build_functions = {\n",
    "    \"Input\":build_input,\n",
    "    \"default\":build_default,\n",
    "    \"Model\":build_model,\n",
    "    \"Compile\":build_compile,\n",
    "    \"Train\":build_train\n",
    "}\n",
    "\n",
    "build = ''\n",
    "\n",
    "for level in levels:\n",
    "    for layer in level:\n",
    "        layer = build_config[layer]\n",
    "        if layer['type'] in build_functions: \n",
    "            build += build_functions[layer['type']](layer,build_config) + '\\n\\n'\n",
    "        else:\n",
    "            build += build_functions['default'](layer,build_config) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-*- Code generated by Tensorflow GUI -*-\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers,optimizers,losses,metrics\n",
      "\n",
      "#<dataset id=mnist_1>\n",
      "class Dataset:\n",
      "    \"\"\"\n",
      "    Dataset will be used in training \n",
      "\n",
      "    The dataset object needs to have following attributes\n",
      "\n",
      "    train_x : np.ndarray -> Training features\n",
      "    train_y : np.ndarray -> Training labels \n",
      "    test_x : np.ndarray -> Testing features\n",
      "    test_y : np.ndarray -> Testing labels\n",
      "\n",
      "    validate : bool -> Weather use validation data or not\n",
      "\n",
      "    batch_size : int -> Batch size\n",
      "    epochs : int -> Number of epochs\n",
      "    batches : int -> Number of batches ( Will be calculated automatically )\n",
      "    \"\"\"\n",
      "    train_x = None\n",
      "    test_x = None\n",
      "    train_y = None\n",
      "    test_y = None\n",
      "\n",
      "    validate = True\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        \"\"\"\n",
      "        Load dataset and set required variables.\n",
      "        \"\"\"\n",
      "\n",
      "        (X,Y),(x,y) = keras.datasets.mnist.load_data()\n",
      "\n",
      "        self.train_x = X.reshape(-1,784) / 255\n",
      "        self.train_y = keras.utils.to_categorical(Y)\n",
      "        self.test_x = X.reshape(-1,784) / 255\n",
      "        self.test_y = keras.utils.to_categorical(Y)\n",
      "\n",
      "#</dataset>\n",
      "                    \n",
      "dataset = Dataset()\n",
      "\n",
      "input_1 = layers.Input(\n",
      "    shape=(784,),\n",
      "    batch_size=None,\n",
      "    name=None,\n",
      "    dtype=None,\n",
      "    sparse=False,\n",
      "    tensor=None,\n",
      "    ragged=False,\n",
      ")\n",
      "\n",
      "dense_1 = layers.Dense(\n",
      "    units=10,\n",
      "    activation=None,\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(input_1)\n",
      "\n",
      "batchnormalization_1 = layers.BatchNormalization(\n",
      "    momentum=0.99,\n",
      "    epsilon=0.001,\n",
      "    center=True,\n",
      "    scale=True,\n",
      "    beta_regularizer=None,\n",
      "    gamma_regularizer=None,\n",
      "    beta_constraint=None,\n",
      "    gamma_constraint=None,\n",
      "    renorm=False,\n",
      "    renorm_clipping=None,\n",
      "    renorm_momentum=0.99,\n",
      "    fused=None,\n",
      "    trainable=True,\n",
      "    virtual_batch_size=None,\n",
      "    adjustment=None,\n",
      "    name=None,\n",
      ")(dense_1)\n",
      "\n",
      "activation_1 = layers.Activation(\n",
      "    activation='softmax',\n",
      ")(batchnormalization_1)\n",
      "\n",
      "model_1 = keras.Model(\n",
      "    [ input_1, ],\n",
      "    [ activation_1, ]\n",
      ")\n",
      "\n",
      "model_1.compile(\n",
      "    optimizer=adam_1,\n",
      "    loss='categorical_crossentropy'\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"#-*- Code generated by Tensorflow GUI -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,optimizers,losses,metrics\n",
    "\n",
    "{dataset}\n",
    "dataset = Dataset()\n",
    "\n",
    "{build}\n",
    "\"\"\".format(\n",
    "    dataset=build_config[train_config['dataset']]['arguments']['dataset']['value'][225:],\n",
    "    build=build\n",
    ")\n",
    "\n",
    "print (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
