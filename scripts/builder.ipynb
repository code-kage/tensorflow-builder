{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load,dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inbound(inbound:list)->str:\n",
    "    return ( \"([\" + \", \".join(inbound) + \"])\" if len(inbound) > 1 else \"(\" + inbound[0] + \")\" ) if len(inbound) else \"\"\n",
    "\n",
    "def set_argument(argument,config):\n",
    "    value = config['value']\n",
    "    try:\n",
    "        value = eval(value)\n",
    "    except:\n",
    "        pass\n",
    "    value = ( None if value == 'None' else f\"'{value}'\" ) if config['type'] == 'text' else value\n",
    "    return f\"    {argument}={value.__repr__()},\"\n",
    "    \n",
    "def build_arguments(arguments:dict)->str:\n",
    "    arguments = '\\n'.join([ set_argument(arg,cnf) for arg,cnf in arguments.items() ])\n",
    "    return arguments\n",
    "\n",
    "def build_default(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound']) if layer['type']['name'] != 'Input' else '' \n",
    "    \n",
    "    return f\"\"\"{layer['id']} = {layer['type']['_class']}.{layer['type']['name']}(\n",
    "{arguments}\n",
    "){inbound} #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_application(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound']) if layer['type']['name'] != 'Input' else '' \n",
    "    return f\"\"\"{layer['id']} = applications.{layer['type']['_class']}(\n",
    "    input_tensor={layer['connections']['inbound'][0]},\n",
    "{arguments}\n",
    "    include_top=False\n",
    ").output #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_model(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['model'] = layer\n",
    "    return f\"\"\"{layer['id']} = keras.Model(\n",
    "    [ {', '.join(build_config['input_nodes'])}, ],\n",
    "    [ {', '.join(layer['connections']['inbound'])}, ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_compile(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['compile'] = layer\n",
    "    model,*_ = [node for node in layer['connections']['inbound'] if \"model\" in node]\n",
    "    metrics = layer['arguments']['metrics']['value']\n",
    "    metrics = \"[\\\"\" + '\", \"'.join(metrics) + \"\\\"]\" if len(metrics) else 'None'\n",
    "    \n",
    "    train_config = build_config['train_config']\n",
    "    return f\"\"\"{train_config['optimizer']['value'] if train_config['optimizer'] else ''}\n",
    "{model}.compile(\n",
    "    optimizer={train_config['optimizer']['id'] if train_config['optimizer'] else \"'\"+layer['arguments']['optmizer']['value']+\"'\"},\n",
    "    loss='{layer['arguments']['loss']['value']}',\n",
    "    metrics={metrics}\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_train(layer,build_config,*args,**kwargs)->str:\n",
    "    callbacks = [callback['value'] for callback in build_config['train_config']['callbacks']]\n",
    "    callback_ids = [callback['id'] for callback in build_config['train_config']['callbacks']]\n",
    "    build_config['train_config']['train'] = layer\n",
    "        \n",
    "    print (build_config['train_config']['model'])\n",
    "        \n",
    "    return f\"\"\"{\"\".join(callbacks)}\n",
    "{build_config['train_config']['model']['id']}.fit(\n",
    "    x={build_config['train_config']['dataset']['id']}.train_x,\n",
    "    y={build_config['train_config']['dataset']['id']}.train_y,\n",
    "    batch_size={layer['arguments']['batch_size']['value']},\n",
    "    epochs={layer['arguments']['epochs']['value']},\n",
    "    validation_data=( {build_config['train_config']['dataset']['id']}.test_x, {build_config['train_config']['dataset']['id']}.test_y ),\n",
    "    callbacks=[ tfgui, {', '.join(callback_ids)} ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "build_functions = {\n",
    "    \"default\":build_default,\n",
    "    \"Model\":build_model,\n",
    "    \"Compile\":build_compile,\n",
    "    \"Train\":build_train,\n",
    "    \"Application\":build_application\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'model_1', 'name': 'Model 1', 'type': {'name': 'Model', '_class': 'models'}, 'pos': {'x': 197, 'y': 554, 'offsetX': 42, 'offsetY': 20}, 'connections': {'inbound': ['dense_1'], 'outbound': ['compile_1']}, 'width': 84, 'arguments': {}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../temp/graph.json\",\"r\") as build_file:\n",
    "    build_config = load(build_file)\n",
    "    \n",
    "inputs = []\n",
    "train_config = {\n",
    "    \"dataset\":None,\n",
    "    \"optimizer\":None,\n",
    "    \"loss\":None,\n",
    "    \"callbacks\":[],\n",
    "    \n",
    "    \"model\":None,\n",
    "    \"compile\":None,\n",
    "    \"train\":None\n",
    "}\n",
    "\n",
    "\n",
    "for _id,config in build_config.items():\n",
    "#     print (config['type']['name'])\n",
    "    if config['type']['name'] == 'Input':\n",
    "        inputs.append(_id)\n",
    "        \n",
    "    elif config['type']['name'] == 'Dataset':\n",
    "        train_config['dataset'] = _id\n",
    "             \n",
    "    elif config['type']['name'] == 'Model':\n",
    "        train_config['model'] = _id\n",
    "        \n",
    "    elif config['type']['name'] == 'Loss':\n",
    "        train_config['loss'] = _id\n",
    "    \n",
    "    if config['type']['_class'] == 'optimizers':\n",
    "        train_config['optimizer'] = _id\n",
    "        \n",
    "    elif config['type']['_class'] == 'callbacks':\n",
    "        train_config['callbacks'].append(_id)\n",
    "   \n",
    "\n",
    "build_config['train_config'] = train_config\n",
    "build_config['input_nodes'] = inputs\n",
    "levels = [ set() for i in range(len(build_config))]\n",
    "def setLevel(node,config,di=0):\n",
    "    levels[di].add(node)\n",
    "    if build_config[node]['connections']['outbound']:\n",
    "        for next_node in build_config[node]['connections']['outbound']: \n",
    "            setLevel(next_node,build_config,di+1)\n",
    "\n",
    "for inp in inputs:\n",
    "    setLevel(inp,build_config,0)\n",
    "    \n",
    "build_config['levels'] = levels\n",
    "levels = [list(level) for level in levels if len(level)]\n",
    "for key,val in train_config.items():\n",
    "    if val != None:\n",
    "        if key == 'dataset':\n",
    "            train_config['dataset'] = {\n",
    "                \"id\":val,\n",
    "                \"value\":build_config[val]['arguments']['dataset']['value']\n",
    "            }\n",
    "        elif key == 'optimizer':\n",
    "            train_config['optimizer'] = {\n",
    "                'id':val,\n",
    "                'value':build_default(build_config[val],build_config)+'\\n'\n",
    "            }\n",
    "        elif key == 'callbacks':\n",
    "            callbacks = []\n",
    "            for callback in train_config['callbacks']:\n",
    "                callbacks.append({\n",
    "                    \"id\":callback,\n",
    "                    \"value\":build_default(build_config[callback],build_config)\n",
    "                })\n",
    "            train_config['callbacks'] = callbacks\n",
    "            \n",
    "build = ''\n",
    "\n",
    "for level in levels:\n",
    "    for layer in level:\n",
    "        layer = build_config[layer]\n",
    "        if layer['type']['name'] in build_functions: \n",
    "            build += build_functions[layer['type']['name']](layer,build_config) + '\\n\\n'\n",
    "        else:\n",
    "            build += build_functions['default'](layer,build_config) + '\\n\\n'\n",
    "            \n",
    "build = build[:-2]\n",
    "code = \"\"\"#-*- Code generated by Tensorflow GUI -*-\n",
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications\n",
    "\n",
    "#end-import\n",
    "\n",
    "{dataset}\n",
    "{build}\n",
    "\"\"\".format(\n",
    "    dataset=train_config['dataset']['value'],\n",
    "    build=build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_train.json\",\"w+\") as file:\n",
    "    file.write(build_config.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-*- Code generated by Tensorflow GUI -*-\n",
      "#import\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications\n",
      "\n",
      "#end-import\n",
      "\n",
      "\"\"\"\n",
      "Note : Don't change dataset id.\n",
      "\n",
      "All the required packages have been imported with their standard namespaces.\n",
      "\n",
      "tensorflow as tf\n",
      "keras as keras\n",
      "pandas as pd\n",
      "numpy as np\n",
      "\n",
      "from sklearn.model_selection , train_test_split\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "#dataset id=dataset_1\n",
      "class Dataset:\n",
      "    \"\"\"\n",
      "    Dataset will be used in training \n",
      "\n",
      "    The dataset object needs to have following attributes\n",
      "\n",
      "    train_x : np.ndarray -> Training features\n",
      "    train_y : np.ndarray -> Training labels \n",
      "    test_x : np.ndarray -> Testing features\n",
      "    test_y : np.ndarray -> Testing labels\n",
      "\n",
      "    validate : bool -> Weather use validation data or not\n",
      "\n",
      "    batch_size : int -> Batch size\n",
      "    epochs : int -> Number of epochs\n",
      "    batches : int -> Number of batches ( Will be calculated automatically )\n",
      "    \"\"\"\n",
      "    train_x = None\n",
      "    test_x = None\n",
      "    train_y = None\n",
      "    test_y = None\n",
      "\n",
      "    validate = True\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        \"\"\"\n",
      "        Load dataset and set required variables.\n",
      "        \"\"\"\n",
      "\n",
      "        self.train_x = None\n",
      "        self.train_y = None\n",
      "        self.test_x = None\n",
      "        self.test_y = None\n",
      "\n",
      "        self.x_shape = ( 224, 224, 3)\n",
      "\n",
      "# Do not change the anything.\n",
      "dataset_1 = Dataset()\n",
      "#end-dataset id=dataset_1\n",
      "\n",
      "input_1 = layers.Input(\n",
      "    shape=(224, 224, 3),\n",
      "    batch_size=None,\n",
      "    name=None,\n",
      "    dtype=None,\n",
      "    sparse=False,\n",
      "    tensor=None,\n",
      "    ragged=False,\n",
      ") #end-input_1\n",
      "\n",
      "\n",
      "mobilenet_1 = applications.MobileNet(\n",
      "    input_tensor=input_1,\n",
      "    weights='imagenet',\n",
      "    include_top=False\n",
      ").output #end-mobilenet_1\n",
      "\n",
      "\n",
      "globalmaxpooling2d_1 = layers.GlobalMaxPooling2D(\n",
      "    data_format=None,\n",
      ")(mobilenet_1) #end-globalmaxpooling2d_1\n",
      "\n",
      "\n",
      "dense_1 = layers.Dense(\n",
      "    units=1000,\n",
      "    activation='softmax',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(globalmaxpooling2d_1) #end-dense_1\n",
      "\n",
      "\n",
      "model_1 = keras.Model(\n",
      "    [ input_1, ],\n",
      "    [ dense_1, ]\n",
      ") #end-model_1\n",
      "\n",
      "\n",
      "\n",
      "model_1.compile(\n",
      "    optimizer='adam',\n",
      "    loss='categorical_crossentropy',\n",
      "    metrics=[\"categorical_accuracy\"]\n",
      ") #end-compile_1\n",
      "\n",
      "\n",
      "\n",
      "model_1.fit(\n",
      "    x=dataset_1.train_x,\n",
      "    y=dataset_1.train_y,\n",
      "    batch_size=32,\n",
      "    epochs=3,\n",
      "    validation_data=( dataset_1.test_x, dataset_1.test_y ),\n",
      "    callbacks=[ tfgui,  ]\n",
      ") #end-train_1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_code.py\",\"w+\") as file:\n",
    "    file.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      " 15138816/553467096 [..............................] - ETA: 11:53"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-191a7de453b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m       weights_path = data_utils.get_file(\n\u001b[0m\u001b[0;32m    212\u001b[0m           \u001b[1;34m'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mWEIGHTS_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\py38\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.applications.VGG16().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
