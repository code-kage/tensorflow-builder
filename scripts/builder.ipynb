{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load,dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inbound(inbound:list)->str:\n",
    "    return ( \"([\" + \", \".join(inbound) + \"])\" if len(inbound) > 1 else \"(\" + inbound[0] + \")\" ) if len(inbound) else \"\"\n",
    "\n",
    "def set_argument(argument,config):\n",
    "    value = config['value']\n",
    "    try:\n",
    "        value = eval(value)\n",
    "    except:\n",
    "        pass\n",
    "    value = ( None if value == 'None' else f\"'{value}'\" ) if config['type'] == 'text' else value\n",
    "    return f\"    {argument}={value.__repr__()},\"\n",
    "    \n",
    "def build_arguments(arguments:dict)->str:\n",
    "    arguments = '\\n'.join([ set_argument(arg,cnf) for arg,cnf in arguments.items() ])\n",
    "    return arguments\n",
    "\n",
    "def build_default(layer,build_config,*args,**kwargs)->str:\n",
    "    arguments =  build_arguments(layer['arguments'])\n",
    "    inbound = build_inbound(layer['connections']['inbound']) if layer['type']['name'] != 'Input' else '' \n",
    "    \n",
    "    return f\"\"\"{layer['id']} = {layer['type']['_class']}.{layer['type']['name']}(\n",
    "{arguments}\n",
    "){inbound} #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_model(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['model'] = layer\n",
    "    return f\"\"\"{layer['id']} = keras.Model(\n",
    "    [ {', '.join(build_config['input_nodes'])}, ],\n",
    "    [ {', '.join(layer['connections']['inbound'])}, ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_compile(layer,build_config,*args,**kwargs)->str:\n",
    "    build_config['train_config']['compile'] = layer\n",
    "    model,*_ = [node for node in layer['connections']['inbound'] if \"model\" in node]\n",
    "    metrics = layer['arguments']['metrics']['value']\n",
    "    \n",
    "    train_config = build_config['train_config']\n",
    "    return f\"\"\"{train_config['optimizer']['value'] if train_config['optimizer'] else ''}\n",
    "{model}.compile(\n",
    "    optimizer={train_config['optimizer']['id'] if train_config['optimizer'] else \"'\"+layer['arguments']['optmizer']['value']+\"'\"},\n",
    "    loss='{layer['arguments']['loss']['value']}',\n",
    "    metrics=[ {', '.join(metrics)}, ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "def build_train(layer,build_config,*args,**kwargs)->str:\n",
    "    callbacks = [callback['value'] for callback in build_config['train_config']['callbacks']]\n",
    "    callback_ids = [callback['id'] for callback in build_config['train_config']['callbacks']]\n",
    "    build_config['train_config']['train'] = layer\n",
    "    \n",
    "    return f\"\"\"{\"\".join(callbacks)}\n",
    "{build_config['train_config']['model']['id']}.fit(\n",
    "    x={build_config['train_config']['dataset']['id']}.train_x,\n",
    "    y={build_config['train_config']['dataset']['id']}.train_y,\n",
    "    batch_size={layer['arguments']['batch_size']['value']},\n",
    "    epochs={layer['arguments']['epochs']['value']},\n",
    "    callbacks=[ tfgui, {', '.join(callback_ids)} ]\n",
    ") #end-{layer['id']}\n",
    "\"\"\"\n",
    "\n",
    "build_functions = {\n",
    "    \"default\":build_default,\n",
    "    \"Model\":build_model,\n",
    "    \"Compile\":build_compile,\n",
    "    \"Train\":build_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "ModelCheckpoint\n",
      "TensorBoard\n",
      "Adam\n",
      "Dense\n",
      "GlobalAveragePooling2D\n",
      "Conv2D\n",
      "Conv2D\n",
      "Input\n",
      "Dataset\n",
      "Compile\n",
      "Train\n"
     ]
    }
   ],
   "source": [
    "with open(\"../temp/build.json\",\"r\") as build_file:\n",
    "    build_config = load(build_file)\n",
    "    \n",
    "inputs = []\n",
    "train_config = {\n",
    "    \"dataset\":None,\n",
    "    \"optimizer\":None,\n",
    "    \"loss\":None,\n",
    "    \"callbacks\":[],\n",
    "    \n",
    "    \"model\":None,\n",
    "    \"compile\":None,\n",
    "    \"train\":None\n",
    "}\n",
    "\n",
    "\n",
    "for _id,config in build_config.items():\n",
    "    print (config['type']['name'])\n",
    "    if config['type']['name'] == 'Input':\n",
    "        inputs.append(_id)\n",
    "        \n",
    "    elif config['type']['name'] == 'Dataset':\n",
    "        train_config['dataset'] = _id\n",
    "             \n",
    "    elif config['type']['name'] == 'Model':\n",
    "        train_config['model'] = _id\n",
    "        \n",
    "    elif config['type']['name'] == 'Loss':\n",
    "        train_config['loss'] = _id\n",
    "    \n",
    "    if config['type']['_class'] == 'optimizers':\n",
    "        train_config['optimizer'] = _id\n",
    "        \n",
    "    elif config['type']['_class'] == 'callbacks':\n",
    "        train_config['callbacks'].append(_id)\n",
    "   \n",
    "\n",
    "build_config['train_config'] = train_config\n",
    "build_config['input_nodes'] = inputs\n",
    "levels = [ set() for i in range(len(build_config))]\n",
    "def setLevel(node,config,di=0):\n",
    "    levels[di].add(node)\n",
    "    if build_config[node]['connections']['outbound']:\n",
    "        for next_node in build_config[node]['connections']['outbound']: \n",
    "            setLevel(next_node,build_config,di+1)\n",
    "\n",
    "for inp in inputs:\n",
    "    setLevel(inp,build_config,0)\n",
    "    \n",
    "build_config['levels'] = levels\n",
    "levels = [list(level) for level in levels if len(level)]\n",
    "for key,val in train_config.items():\n",
    "    if val != None:\n",
    "        if key == 'dataset':\n",
    "            train_config['dataset'] = {\n",
    "                \"id\":val,\n",
    "                \"value\":build_config[val]['arguments']['dataset']['value']\n",
    "            }\n",
    "        elif key == 'optimizer':\n",
    "            train_config['optimizer'] = {\n",
    "                'id':val,\n",
    "                'value':build_default(build_config[val],build_config)+'\\n'\n",
    "            }\n",
    "        elif key == 'callbacks':\n",
    "            callbacks = []\n",
    "            for callback in train_config['callbacks']:\n",
    "                callbacks.append({\n",
    "                    \"id\":callback,\n",
    "                    \"value\":build_default(build_config[callback],build_config)\n",
    "                })\n",
    "            train_config['callbacks'] = callbacks\n",
    "            \n",
    "build = ''\n",
    "\n",
    "for level in levels:\n",
    "    for layer in level:\n",
    "        layer = build_config[layer]\n",
    "        if layer['type']['name'] in build_functions: \n",
    "            build += build_functions[layer['type']['name']](layer,build_config) + '\\n\\n'\n",
    "        else:\n",
    "            build += build_functions['default'](layer,build_config) + '\\n\\n'\n",
    "            \n",
    "build = build[:-2]\n",
    "code = \"\"\"#-*- Code generated by Tensorflow GUI -*-\n",
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks\n",
    "#end-import\n",
    "\n",
    "{dataset}\n",
    "{build}\n",
    "\"\"\".format(\n",
    "    dataset=train_config['dataset']['value'],\n",
    "    build=build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_train.json\",\"w+\") as file:\n",
    "    file.write(build_config.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-*- Code generated by Tensorflow GUI -*-\n",
      "#import\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks\n",
      "#end-import\n",
      "\n",
      "\"\"\"\n",
      "Note : Don't change dataset id.\n",
      "All the required packages have been imported with their standard namespaces.\n",
      "\n",
      "tensorflow as tf\n",
      "keras as keras\n",
      "pandas as pd\n",
      "numpy as np\n",
      "\n",
      "from sklearn.model_selection , train_test_split\n",
      "\"\"\"\n",
      "\n",
      "#dataset id=mnist_2\n",
      "class Dataset:\n",
      "    \"\"\"\n",
      "    Dataset will be used in training \n",
      "\n",
      "    The dataset object needs to have following attributes\n",
      "\n",
      "    train_x : np.ndarray -> Training features\n",
      "    train_y : np.ndarray -> Training labels \n",
      "    test_x : np.ndarray -> Testing features\n",
      "    test_y : np.ndarray -> Testing labels\n",
      "\n",
      "    validate : bool -> Weather use validation data or not\n",
      "\n",
      "    batch_size : int -> Batch size\n",
      "    epochs : int -> Number of epochs\n",
      "    batches : int -> Number of batches ( Will be calculated automatically )\n",
      "    \"\"\"\n",
      "    train_x = None\n",
      "    test_x = None\n",
      "    train_y = None\n",
      "    test_y = None\n",
      "\n",
      "    validate = True\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        \"\"\"\n",
      "        Load dataset and set required variables.\n",
      "        \"\"\"\n",
      "\n",
      "        (X,Y),(x,y) = keras.datasets.mnist.load_data()\n",
      "\n",
      "        self.train_x = X.reshape(-1,28, 28, 1) / 255\n",
      "        self.train_y = keras.utils.to_categorical(Y)\n",
      "        self.test_x = X.reshape(-1,28, 28, 1) / 255\n",
      "        self.test_y = keras.utils.to_categorical(Y)\n",
      "    \n",
      "# Do not change the anything.\n",
      "mnist_2 = Dataset()\n",
      "#end-dataset id=mnist_2\n",
      "                    \n",
      "input_3 = layers.Input(\n",
      "    shape=(28, 28, 1),\n",
      "    batch_size=None,\n",
      "    name=None,\n",
      "    dtype=None,\n",
      "    sparse=False,\n",
      "    tensor=None,\n",
      "    ragged=False,\n",
      ") #end-input_3\n",
      "\n",
      "\n",
      "conv2d_4 = layers.Conv2D(\n",
      "    filters=32,\n",
      "    kernel_size=3,\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='relu',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(input_3) #end-conv2d_4\n",
      "\n",
      "\n",
      "conv2d_3 = layers.Conv2D(\n",
      "    filters=64,\n",
      "    kernel_size=3,\n",
      "    strides=2,\n",
      "    data_format=None,\n",
      "    dilation_rate=(1, 1),\n",
      "    groups=1,\n",
      "    activation='relu',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(conv2d_4) #end-conv2d_3\n",
      "\n",
      "\n",
      "globalaveragepooling2d_2 = layers.GlobalAveragePooling2D(\n",
      "    data_format=None,\n",
      ")(conv2d_3) #end-globalaveragepooling2d_2\n",
      "\n",
      "\n",
      "dense_2 = layers.Dense(\n",
      "    units=10,\n",
      "    activation='softmax',\n",
      "    use_bias=True,\n",
      "    kernel_regularizer=None,\n",
      "    bias_regularizer=None,\n",
      "    activity_regularizer=None,\n",
      "    kernel_constraint=None,\n",
      "    bias_constraint=None,\n",
      ")(globalaveragepooling2d_2) #end-dense_2\n",
      "\n",
      "\n",
      "model_1 = keras.Model(\n",
      "    [ input_3, ],\n",
      "    [ dense_2, ]\n",
      ") #end-model_1\n",
      "\n",
      "\n",
      "adam_1 = optimizers.Adam(\n",
      "    learning_rate=0.001,\n",
      "    beta_1=0.9,\n",
      "    beta_2=0.999,\n",
      "    epsilon=1e-07,\n",
      "    amsgrad=False,\n",
      "    name='Adam',\n",
      ") #end-adam_1\n",
      "\n",
      "\n",
      "model_1.compile(\n",
      "    optimizer=adam_1,\n",
      "    loss='None',\n",
      "    metrics=[ categorical_accuracy, categorical_crossentropy ]\n",
      ") #end-compile_2\n",
      "\n",
      "\n",
      "modelcheckpoint_1 = callbacks.ModelCheckpoint(\n",
      "    filepath='./temp/model',\n",
      "    monitor='val_loss',\n",
      "    verbose=0,\n",
      "    save_best_only=False,\n",
      "    save_weights_only=False,\n",
      "    mode='auto',\n",
      "    save_freq='epoch',\n",
      "    options=None,\n",
      ") #end-modelcheckpoint_1\n",
      "tensorboard_1 = callbacks.TensorBoard(\n",
      "    log_dir='./temp/logs',\n",
      "    histogram_freq=0,\n",
      "    write_graph=True,\n",
      "    write_images=False,\n",
      "    update_freq='epoch',\n",
      "    profile_batch=2,\n",
      "    embeddings_freq=0,\n",
      "    embeddings_metadata=None,\n",
      ") #end-tensorboard_1\n",
      "\n",
      "model_1.fit(\n",
      "    x=mnist_2.train_x,\n",
      "    y=mnist_2.train_y,\n",
      "    batch_size=8,\n",
      "    epochs=1,\n",
      "    callbacks=[ tfgui, modelcheckpoint_1, tensorboard_1 ]\n",
      ") #end-train_2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/example_code.py\",\"w+\") as file:\n",
    "    file.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_config['train_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
