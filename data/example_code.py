#-*- Code generated by Tensorflow GUI -*-
#import
import pandas as pd
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers,optimizers,losses,metrics,callbacks
#end-import

"""
Note : Don't change dataset id.
All the required packages have been imported with their standard namespaces.

tensorflow as tf
keras as keras
pandas as pd
numpy as np

from sklearn.model_selection , train_test_split
"""

#dataset id=cifar10_1
class Dataset:
    """
    Dataset will be used in training 

    The dataset object needs to have following attributes

    train_x : np.ndarray -> Training features
    train_y : np.ndarray -> Training labels 
    test_x : np.ndarray -> Testing features
    test_y : np.ndarray -> Testing labels

    validate : bool -> Weather use validation data or not

    batch_size : int -> Batch size
    epochs : int -> Number of epochs
    batches : int -> Number of batches ( Will be calculated automatically )
    """
    train_x = None
    test_x = None
    train_y = None
    test_y = None

    validate = True

    def __init__(self) -> None:
        """
        Load dataset and set required variables.
        """
        
        (X,Y),(x,y)  = keras.datasets.cifar10.load_data()
        
        self.train_x = X.reshape(-1, 32, 32, 3) / 255
        self.train_y = keras.utils.to_categorical(Y)
        self.test_x = x.reshape(-1, 32, 32, 3) / 255
        self.test_y = keras.utils.to_categorical(y)
        
        self.x_shape = (32, 32, 3)
    
# Do not change the anything.
cifar10_1 = Dataset()
#end-dataset id=cifar10_1
                    
input_1 = layers.Input(
    shape=(32, 32, 3),
    batch_size=None,
    name=None,
    dtype=None,
    sparse=False,
    tensor=None,
    ragged=False,
) #end-input_1


conv2d_1 = layers.Conv2D(
    filters=200,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(input_1) #end-conv2d_1


conv2d_2 = layers.Conv2D(
    filters=180,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(conv2d_1) #end-conv2d_2


maxpooling2d_1 = layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=None,
    data_format=None,
)(conv2d_2) #end-maxpooling2d_1


conv2d_3 = layers.Conv2D(
    filters=180,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(maxpooling2d_1) #end-conv2d_3


conv2d_4 = layers.Conv2D(
    filters=140,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(conv2d_3) #end-conv2d_4


conv2d_5 = layers.Conv2D(
    filters=100,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(conv2d_4) #end-conv2d_5


conv2d_6 = layers.Conv2D(
    filters=50,
    kernel_size=3,
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(conv2d_5) #end-conv2d_6


maxpooling2d_2 = layers.MaxPooling2D(
    pool_size=(2, 2),
    strides=None,
    data_format=None,
)(conv2d_6) #end-maxpooling2d_2


flatten_1 = layers.Flatten(
    data_format=None,
)(maxpooling2d_2) #end-flatten_1


dense_1 = layers.Dense(
    units=180,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(flatten_1) #end-dense_1


dense_2 = layers.Dense(
    units=100,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(dense_1) #end-dense_2


dense_3 = layers.Dense(
    units=50,
    activation='relu',
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(dense_2) #end-dense_3


dropout_1 = layers.Dropout(
    rate=0.5,
    noise_shape=None,
    seed=None,
)(dense_3) #end-dropout_1


dense_6 = layers.Dense(
    units='REQUIRED',
    activation=None,
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(dropout_1) #end-dense_6


model_2 = keras.Model(
    [ input_1, ],
    [ dense_6, ]
) #end-model_2


adam_1 = optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
) #end-adam_1


model_1.compile(
    optimizer=adam_1,
    loss='categorical_crossentropy',
    metrics=["categorical_accuracy"]
) #end-compile_1



model_2.fit(
    x=cifar10_1.train_x,
    y=cifar10_1.train_y,
    batch_size=32,
    epochs=3,
    callbacks=[ tfgui,  ]
) #end-train_1

