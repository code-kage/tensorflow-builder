#-*- Code generated by Tensorflow GUI -*-
#import
import pandas as pd
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications

#end-import

"""
Note : Don't change dataset id.

All the required packages have been imported with their standard namespaces.

tensorflow as tf
keras as keras
pandas as pd
numpy as np

from sklearn.model_selection , train_test_split
"""


#dataset id=dataset_1
class Dataset:
    """
    Dataset will be used in training 

    The dataset object needs to have following attributes

    train_x : np.ndarray -> Training features
    train_y : np.ndarray -> Training labels 
    test_x : np.ndarray -> Testing features
    test_y : np.ndarray -> Testing labels

    validate : bool -> Weather use validation data or not

    batch_size : int -> Batch size
    epochs : int -> Number of epochs
    batches : int -> Number of batches ( Will be calculated automatically )
    """
    train_x = None
    test_x = None
    train_y = None
    test_y = None

    validate = True

    def __init__(self) -> None:
        """
        Load dataset and set required variables.
        """

        images = glob("C:\\workspace\\tensorflow-gui\\data\\datasets\\bfsiw\\leedsbutterfly\\images\\*")
        labels = glob("C:\\workspace\\tensorflow-gui\\data\\datasets\\bfsiw\\leedsbutterfly\\segmentations\\*")

    
        self.train_x = np.zeros((len(images),224,224,3)).astype(np.float32)
        self.train_y = np.zeros((len(labels),224,224,3)).astype(np.float32)

        def get_image(args):
            index,path,array = args
            im = cv2.imread(path,)[:,:,::-1]
            im = cv2.resize(im,(224,224),interpolation=cv2.INTER_AREA)
            array[index] = im
            return 1


        with ThreadPoolExecutor(max_workers=32) as executor:
            res = executor.map(get_image,[ ( i,path,self.train_x ) for i,path in enumerate(images)])
            print (sum(list(res)))
            
        with ThreadPoolExecutor(max_workers=32) as executor:
            res = executor.map(get_image,[ ( i,path,self.train_y ) for i,path in enumerate(labels)])

        self.train_y = self.train_y.mean(axis=-1) / 255
        test_idx = np.random.randint(0,len(self.train_x),size=32)
        
        self.test_x = self.train_x[test_idx]
        self.test_y = self.train_y[test_idx]

        collect()
        
# Do not change the anything.
dataset_1 = Dataset()
#end-dataset id=dataset_1

#node_0
def conv_2d_block(
    inbound:list=[],
    filters:int=8,
    activation:str="swish"
  )->None:
  
  x = layers.Conv2D(filters,kernel_size=3, padding="same")(inbound)
  x = layers.BatchNormalization()(x)
  x = layers.Activation(activation)(x)

  x = layers.Conv2D(filters,kernel_size=3,strides=2,padding="same")(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation(activation)(x)

  x = layers.Dropout(0.1)(x)

  return x  

#end-node_0

#node_1
def convt_2d_block(
    inbound:list=[],
    filters:int=8,
    activation:str="swish"
  )->None:

  x = layers.Concatenate()(inbound)  
  x = layers.Conv2DTranspose(filters,kernel_size=3,padding="same")(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation(activation)(x)

  x = layers.Conv2DTranspose(filters,kernel_size=3,strides=2,padding="same")(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation(activation)(x)

  x = layers.Dropout(0.1)(x)

  return x  

#end-node_1


input_1 = layers.Input(
    shape=(224, 224, 3),
    batch_size=None,
    name=None,
    dtype=None,
    sparse=False,
    tensor=None,
    ragged=False,
) #end-input_1


conv_block_1 = conv_2d_block(
    inbound=input_1,
    filters=8,
    activation='swish',
) #end-conv_block_1


conv_block_2 = conv_2d_block(
    inbound=conv_block_1,
    filters=16,
    activation='swish',
) #end-conv_block_2


conv_block_3 = conv_2d_block(
    inbound=conv_block_2,
    filters=32,
    activation='swish',
) #end-conv_block_3


conv_block_4 = conv_2d_block(
    inbound=conv_block_3,
    filters=64,
    activation='swish',
) #end-conv_block_4


dense_1 = layers.Dense(
    units=128,
    activation='sigmoid',
    use_bias=False,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(conv_block_4) #end-dense_1


convtranspose_block_1 = convt_2d_block(
    inbound=[ dense_1, conv_block_4 ],
    filters=64,
    activation='swish',
) #end-convtranspose_block_1


convtranspose_block_2 = convt_2d_block(
    inbound=[ convtranspose_block_1, conv_block_3 ],
    filters=32,
    activation='swish',
) #end-convtranspose_block_2


convtranspose_block_3 = convt_2d_block(
    inbound=[ conv_block_2, convtranspose_block_2 ],
    filters=16,
    activation='swish',
) #end-convtranspose_block_3


convtranspose_block_4 = convt_2d_block(
    inbound=[ conv_block_1, convtranspose_block_3 ],
    filters=8,
    activation='swish',
) #end-convtranspose_block_4


conv2d_1 = layers.Conv2D(
    filters=1,
    kernel_size=3,
    padding='same',
    strides=(1, 1),
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
)(convtranspose_block_4) #end-conv2d_1


batchnormalization_1 = layers.BatchNormalization(
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    trainable=True,
    virtual_batch_size=None,
    adjustment=None,
    name=None,
)(conv2d_1) #end-batchnormalization_1


activation_1 = layers.Activation(
    activation='sigmoid',
)(batchnormalization_1) #end-activation_1


model_1 = keras.Model(
    [ input_1, ],
    [ activation_1, ]
) #end-model_1


adam_1 = optimizers.Adam(
    learning_rate=9e-05,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
) #end-adam_1


model_1.compile(
    optimizer=adam_1,
    loss='mean_absolute_error',
    metrics=None
) #end-compile_1



model_1.fit(
    x=dataset_1.train_x,
    y=dataset_1.train_y,
    batch_size=16,
    epochs=3,
    validation_data=( dataset_1.test_x, dataset_1.test_y ),
    callbacks=[ tfgui,  ]
) #end-train_1

